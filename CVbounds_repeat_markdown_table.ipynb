{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <h3> The code to generate Table 1  </h3> </center>\n",
    "## <center> Ning Xu</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is compiled and generated automatically by Jupyter notebook. In this file, we demonstrate repeated simulations on the upper bound. The code in this file is almost the same as the supplementray file \"CVbounds_markdown\". In this file, given each value of $(n/K)/p$, we repeat the computation of the 90% upper bound on different i.i.d. samples 20 times. The results are attached below the codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: define all the functions as we did in the file \"CVbounds\"\n",
    "\n",
    "For more information, please see the file \"CVbounds\" or the simulation section in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter notebook returns the results (figures, outcomes, prints) below the code box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the memory before each compilation\n",
    "%reset -f \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate data for the regression\n",
    "\n",
    "def data_gen(sample_size, n_dim, n_info, cov_noise):\n",
    "    \n",
    "    a = np.ones((n_dim, n_dim)) * 0.5; A = np.eye(n_dim)*0.5\n",
    "\n",
    "    cov_x = a + A; mean_x = np.zeros(n_dim)\n",
    "\n",
    "    X = np.random.multivariate_normal(mean_x, cov_x, sample_size)\n",
    "\n",
    "    beta_info = np.arange(3,n_info + 3)\n",
    "    beta = np.concatenate((beta_info, np.zeros(n_dim-n_info)), axis = 0)\n",
    "\n",
    "    noise = np.random.normal(0, cov_noise, sample_size); \n",
    "    noise.shape = (sample_size, 1); beta.shape = (1, n_dim) \n",
    "\n",
    "    Y = np.inner(X,beta) + noise \n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to estimate the Rademacher complexity, we generate rademacher variables \n",
    "\n",
    "def rade_generator( sample_size ):\n",
    "    \n",
    "    ans = np.random.randint(2, size=sample_size)\n",
    "    rade = (ans - 0.5) * 2\n",
    "    \n",
    "    return rade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimating the empirical rademacher complexity\n",
    "\n",
    "def emp_rc(sample_size, n_iter, reg, n_dim, n_info, cov_noise):\n",
    "    \n",
    "    X, Y = data_gen(sample_size, n_dim, n_info, cov_noise)\n",
    "    \n",
    "    cond = np.zeros([n_iter,1])\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        \n",
    "        rade = rade_generator(sample_size)\n",
    "        reg.fit(X, rade) ; rade_pre = reg.predict(X)\n",
    "        rade_pre = np.matrix(rade_pre); cond[i,0] = np.dot(rade_pre, rade)/ sample_size\n",
    "    \n",
    "    emp_rc = np.mean(cond) * 2 \n",
    "   \n",
    "    return emp_rc, cond\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimating the rademacher complexity\n",
    "\n",
    "def est_rc(sample_size, n_iter, reg, n_dim, n_info, cov_noise):\n",
    "      \n",
    "    cond_rc = np.zeros([n_iter])\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        \n",
    "        emp_rc_in, _ = emp_rc(sample_size, n_iter, reg, n_dim, n_info, cov_noise)\n",
    "\n",
    "        cond_rc[i] = emp_rc_in\n",
    "\n",
    "    est_rc = np.mean(cond_rc)\n",
    "    \n",
    "    return est_rc, cond_rc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establishing the upper bound for classifiers\n",
    "\n",
    "def rc_bound(n, M, varpi, K, rc_1, rc_2, ave_in_sample_error):\n",
    "    \n",
    "    ub = ave_in_sample_error + rc_1 + rc_2 + 2 * M * np.sqrt( np.log( 1 / varpi ) / ( n / K ) )\n",
    "    \n",
    "    return ub  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mean square error of the estimator\n",
    "\n",
    "def costs(X_train, Y_train, clf):\n",
    "    \n",
    "    Y_pred_train = clf.predict(X_train); Y_pred_train.shape = Y_train.shape\n",
    "    loss_train = mean_squared_error(Y_train, Y_pred_train)\n",
    "    \n",
    "    res = np.var(Y_train - Y_pred_train)\n",
    "    \n",
    "    return loss_train, res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function to estimate the upper bounds for the lasso\n",
    "\n",
    "def error_bounds(X_train_1, Y_train_1, X_train_2, Y_train_2, \n",
    "              test_size, varpi, n_iter, n_val, n_dim, n_info, cov_noise, start, end, step):\n",
    "    \n",
    "    ##set the placeholders \n",
    "    alpha = np.arange(start, end, step)          # the seq of penalty parameters\n",
    "    error_matrix = np.zeros((n_val, len(alpha))) # the n_val average test errors for each model\n",
    "    upper_array  = np.zeros(len(alpha))          # the seq of 90 percentiles for all models \n",
    "    train_array  = np.zeros(len(alpha))          # the seq of average trainig errors for all models\n",
    "    test_array   = np.zeros(len(alpha))          # the seq of CV-errors for all models\n",
    "    perc_array   = np.zeros(len(alpha))\n",
    "    \n",
    "    for l in range(len(alpha)):\n",
    "        \n",
    "        #given the value of alpha\n",
    "        \n",
    "        #training classifiers\n",
    "        clf  = Lasso(alpha = alpha[l])\n",
    "        clf1 = Lasso(alpha = alpha[l]); clf1.fit(X_train_1, Y_train_1)\n",
    "        clf2 = Lasso(alpha = alpha[l]); clf2.fit(X_train_2, Y_train_2)\n",
    "        clf3 = Lasso(alpha = alpha[l]); clf2.fit(X_train_2, Y_train_2)\n",
    "    \n",
    "        #compute training errors\n",
    "        loss_train_1, res_var_1 = costs(X_train_1, Y_train_1, clf1)\n",
    "        loss_train_2, res_var_2 = costs(X_train_2, Y_train_2, clf2)\n",
    "        train_array[l] = (loss_train_1 + loss_train_2)/2\n",
    "        \n",
    "        #compute test errors\n",
    "        loss_test_1, res_var_3 = costs(X_train_2, Y_train_2, clf1)\n",
    "        loss_test_2, res_var_4 = costs(X_train_1, Y_train_1, clf2)\n",
    "        test_array[l] = (loss_test_1 + loss_test_2)/2\n",
    "        \n",
    "        res_var = max(res_var_3, res_var_4) ; M = res_var * 1.4\n",
    "        \n",
    "\n",
    "        #container of the validation errors\n",
    "        error_val = np.zeros(n_val); \n",
    "\n",
    "        #the loop to compute the out-of-sample errors\n",
    "        for i in range(n_val):\n",
    "\n",
    "            X_val_1, Y_val_1 = data_gen(test_size, n_dim, n_info, cov_noise)\n",
    "            X_val_2, Y_val_2 = data_gen(test_size, n_dim, n_info, cov_noise)\n",
    "\n",
    "            Y_pred_val_1 = clf1.predict(X_val_1) ; loss_test_val_1 = mean_squared_error(Y_val_1 , Y_pred_val_1)\n",
    "            Y_pred_val_2 = clf2.predict(X_val_2) ; loss_test_val_2 = mean_squared_error(Y_val_2 , Y_pred_val_2)\n",
    "\n",
    "            error_val[i] = (loss_test_val_1 + loss_test_val_2)/2\n",
    "        \n",
    "        error_matrix[:,l] = error_val\n",
    "        \n",
    "        sorted_error = np.sort(error_val); perc_array[l] = sorted_error[-1*round(n_val*0.1)]\n",
    "\n",
    "        # estimating the RC_test and RC_train\n",
    "        est_rc_train, cond_rc_SVC_train = est_rc(test_size, n_iter, clf, n_dim, n_info, cov_noise)\n",
    "        est_rc_test, cond_rc_SVC_test = est_rc(test_size, n_iter, clf, n_dim, n_info, cov_noise)\n",
    "\n",
    "        #establish the upper bounds\n",
    "        upper_array[l] = rc_bound( 2 * test_size, M, varpi, K, est_rc_train, est_rc_test, train_array[l])\n",
    "        \n",
    "    lambda_cv_ind  = np.argmin(test_array)\n",
    "    lambda_cub_ind = np.argmin(upper_array)\n",
    "    \n",
    "    lambda_cv  = 1 + lambda_cv_ind  * 0.5\n",
    "    lambda_cub = 1 + lambda_cub_ind * 0.5        \n",
    "    \n",
    "    return lambda_cv, lambda_cub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function to get the result of the variable/model selection\n",
    "\n",
    "def lasso_norm(X12, Y12, X11, Y11, lambda_cv, lambda_cub):\n",
    "    \n",
    "    clf_11 = Lasso(alpha = lambda_cv ); clf_11.fit(X12, Y12)\n",
    "    clf_12 = Lasso(alpha = lambda_cv ); clf_12.fit(X11, Y11)\n",
    "    clf_21 = Lasso(alpha = lambda_cub); clf_21.fit(X12, Y12)\n",
    "    clf_22 = Lasso(alpha = lambda_cub); clf_22.fit(X11, Y11)\n",
    "    \n",
    "    cub_selection_1 = np.nonzero(clf_21.sparse_coef_)[1] ; cub_norm_1 = np.linalg.norm(clf_21.coef_, ord=1)\n",
    "    cub_selection_2 = np.nonzero(clf_22.sparse_coef_)[1] ; cub_norm_2 = np.linalg.norm(clf_22.coef_, ord=1)\n",
    "    \n",
    "    cv_selection_1  = np.nonzero(clf_11.sparse_coef_)[1] ; cv_norm_1  = np.linalg.norm(clf_11.coef_, ord=1)\n",
    "    cv_selection_2  = np.nonzero(clf_12.sparse_coef_)[1] ; cv_norm_2  = np.linalg.norm(clf_12.coef_, ord=1)\n",
    "    \n",
    "    return cub_selection_1, cub_selection_2, cv_selection_1, cv_selection_2, cub_norm_1, cub_norm_2, cv_norm_1, cv_norm_2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_percent(y, position):\n",
    "    # Ignore the passed in position. This has the effect of scaling the default\n",
    "    # tick locations.\n",
    "    s = str(100 * y)\n",
    "\n",
    "    # The percent symbol needs escaping in latex\n",
    "    if matplotlib.rcParams['text.usetex'] is True:\n",
    "        return s + r'$\\%$'\n",
    "    else:\n",
    "        return s + '%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: the repeated simulation of the upper bounds for $n = 200$.\n",
    "\n",
    "We repeat the $n=200$ simulation 20 times using different random seeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In case that you don't want to run 10 hrs CPU computation, we attached \"CVbounds_table.p\" in the zip file. To load everything back, run the following codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"CVbounds_table.p\", \"rb\") as f:\n",
    "    lambda_cv_seq       = pickle.load( f )\n",
    "    lambda_cub_seq      = pickle.load( f )\n",
    "    selection_cv_seq    = pickle.load( f )\n",
    "    selection_cub_seq   = pickle.load( f )\n",
    "    lambda_cv_seq_5     = pickle.load( f )\n",
    "    lambda_cub_seq_5    = pickle.load( f )\n",
    "    selection_cv_seq_5  = pickle.load( f )\n",
    "    selection_cub_seq_5 = pickle.load( f )\n",
    "    lambda_cv_seq_3     = pickle.load( f )\n",
    "    lambda_cub_seq_3    = pickle.load( f )\n",
    "    selection_cv_seq_3  = pickle.load( f )\n",
    "    selection_cub_seq_3 = pickle.load( f )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or you can redo the computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## average test error bounds for the lasso\n",
    "## total sample size : 200\n",
    "\n",
    "n2 = 100; test_size = n2;  #n2 is the size of each fold\n",
    "\n",
    "# generate data\n",
    "\n",
    "varpi = 0.1; K = 2\n",
    "\n",
    "n_iter = 64; n_val = 10000; n_dim = 100; n_info = 5; cov_noise = 1\n",
    "\n",
    "start = 0.1; end = 0.5; step = 0.05; alpha = np.arange(start, end, step)\n",
    "\n",
    "end_seed = 300; step_seed = 5\n",
    "\n",
    "lambda_cv_seq = list(); lambda_cub_seq = list()\n",
    "selection_cv_seq = list();selection_cub_seq = list()\n",
    "\n",
    "for i in range(0, end_seed, step_seed):\n",
    "    \n",
    "    np.random.seed(i)\n",
    "\n",
    "    X12, Y12 = data_gen(n2, n_dim, n_info, cov_noise)\n",
    "\n",
    "    X22, Y22 = data_gen(n2, n_dim, n_info, cov_noise)\n",
    "\n",
    "\n",
    "    # training the model\n",
    "    lambda_cv, lambda_cub = error_bounds(X12, Y12, X22, Y22, test_size, varpi, n_iter, n_val, n_dim, n_info, cov_noise, start, end, step)\n",
    "\n",
    "    cub_selection_1, cub_selection_2, cv_selection_1, cv_selection_2, cub_norm_1, cub_norm_2, cv_norm_1, cv_norm_2 = lasso_norm(X12, Y12, X22, Y22, lambda_cv, lambda_cub)\n",
    "    \n",
    "    selection_cub_seq.append(cub_selection_1); selection_cub_seq.append(cub_selection_2)\n",
    "    selection_cv_seq.append(cv_selection_1); selection_cv_seq.append(cv_selection_2)\n",
    "    \n",
    "    lambda_cub_seq.append(cub_norm_1); lambda_cub_seq.append(cub_norm_2)\n",
    "    lambda_cv_seq.append(cv_norm_1); lambda_cv_seq.append(cv_norm_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can continue the plotting if you load  \"CVbounds_table.p\" or finished the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f1 = plt.figure()\n",
    "plt.hist(lambda_cub_seq,alpha=0.5, label='the $L_1$ norm of the lasso based on $\\lambda_{cub}$')\n",
    "plt.hist(lambda_cv_seq, alpha=0.5, label='the $L_1$ norm of the lasso based on $\\lambda_{CV}$')\n",
    "#pyplot.legend(loc=0)\n",
    "plt.xlim(17, 24)\n",
    "plt.ylim(0, 33)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "f1.savefig(\"n=200.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on average CV select 9.0 variables\n",
      "in all 40 simulations of CV, 90% of 40 select the first 5 variables\n",
      "on average minimizing the upper bounds select 8.125 variables\n",
      "in all 40 simulations of CUB, 90% of 40 select the first 5 variables\n"
     ]
    }
   ],
   "source": [
    "num_variable_cv  = list(); ind_variable_cv  = 0\n",
    "num_variable_cub = list(); ind_variable_cub = 0\n",
    "\n",
    "for i in range(len(selection_cub_seq)):\n",
    "    \n",
    "    item_cub = selection_cub_seq[i]; item_cv = selection_cv_seq[i]\n",
    "    num_variable_cub.append(len(item_cub)); num_variable_cv.append(len(item_cv))\n",
    "    \n",
    "    if (all(x in item_cub for x in [0, 1, 2, 3, 4]) == True):\n",
    "        \n",
    "        ind_variable_cub += 1\n",
    "    \n",
    "    if (all(x in item_cv for x in [0, 1, 2, 3, 4]) == True):\n",
    "        \n",
    "        ind_variable_cv  += 1\n",
    "\n",
    "print(\"on average CV select\",np.mean(num_variable_cv),\"variables\")\n",
    "\n",
    "if (ind_variable_cv >= 0.9 * len(selection_cub_seq)):\n",
    "    print(\"in all 40 simulations of CV, 90% of 40 select the first 5 variables\")\n",
    "    \n",
    "print(\"on average minimizing the upper bounds select\",np.mean(num_variable_cub),\"variables\")\n",
    "\n",
    "if (ind_variable_cub >= 0.9 * len(selection_cub_seq)):\n",
    "    print(\"in all 40 simulations of CUB, 90% of 40 select the first 5 variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the variance of the number of the variable selected by CV is 5.6\n",
      "the variance of the number of the variable selected by CUB is 4.592708333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"the variance of the number of the variable selected by CV is\",np.var(num_variable_cv))\n",
    "print(\"the variance of the number of the variable selected by CUB is\",np.var(num_variable_cub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: the repeated simulation of the upper bounds for $n = 400$.\n",
    "\n",
    "We repeat the $n=400$ simulation 20 times using different random seeds. All the plots are attached after the code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can skip the following computation if you already load  \"CVbounds_table.p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## average test error bounds for the lasso\n",
    "## total sample size : 400\n",
    "\n",
    "n5 = 200; test_size_5 = n5; np.random.seed(50) #n4 is the size of each fold\n",
    "\n",
    "# generate data\n",
    "\n",
    "lambda_cv_seq_5 = list(); lambda_cub_seq_5 = list()\n",
    "selection_cv_seq_5 = list();selection_cub_seq_5 = list()\n",
    "\n",
    "for i in range(0, end_seed, step_seed):\n",
    "    \n",
    "    np.random.seed(i)\n",
    "\n",
    "    X15, Y15 = data_gen(n5, n_dim, n_info, cov_noise)\n",
    "\n",
    "    X25, Y25 = data_gen(n5, n_dim, n_info, cov_noise)\n",
    "\n",
    "    # training the model\n",
    "    \n",
    "    lambda_cv_5, lambda_cub_5 = error_bounds(X15, Y15, X25, Y25, test_size_5, varpi, n_iter, n_val, n_dim, n_info, cov_noise, start, end, step)\n",
    "\n",
    "    cub_selection5_1, cub_selection5_2, cv_selection5_1, cv_selection5_2, cub_norm5_1, cub_norm5_2, cv_norm5_1, cv_norm5_2 = lasso_norm(X15, Y15, X25, Y25, lambda_cv_5, lambda_cub_5)\n",
    "    \n",
    "    selection_cub_seq_5.append(cub_selection5_1); selection_cub_seq_5.append(cub_selection5_2)\n",
    "    selection_cv_seq_5.append(cv_selection5_1); selection_cv_seq_5.append(cv_selection5_2)\n",
    "    \n",
    "    lambda_cub_seq_5.append(cub_norm5_1); lambda_cub_seq_5.append(cub_norm5_2)\n",
    "    lambda_cv_seq_5.append(cv_norm5_1); lambda_cv_seq_5.append(cv_norm5_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f5 = plt.figure()\n",
    "plt.hist(lambda_cub_seq_5,alpha=0.5, label='the $L_1$ norm of the lasso based on $\\lambda_{cub}$')\n",
    "plt.hist(lambda_cv_seq_5, alpha=0.5, label='the $L_1$ norm of the lasso based on $\\lambda_{CV}$')\n",
    "#plt.legend(loc=0)\n",
    "plt.xlim(17, 24)\n",
    "plt.ylim(0, 33)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "f5.savefig(\"n=400.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on average CV select 6.758333333333334 variables\n",
      "in all 40 simulations of CV, 90% of 40 select the first 5 variables\n",
      "on average minimizing the upper bounds select 6.116666666666666 variables\n",
      "in all 40 simulations of CUB, 90% of 40 select the first 5 variables\n"
     ]
    }
   ],
   "source": [
    "num_variable_cv_5  = list(); ind_variable_cv_5  = 0\n",
    "num_variable_cub_5 = list(); ind_variable_cub_5 = 0\n",
    "\n",
    "for i in range(len(selection_cub_seq)):\n",
    "    \n",
    "    item_cub = selection_cub_seq_5[i]; item_cv = selection_cv_seq_5[i]\n",
    "    num_variable_cub_5.append(len(item_cub)); num_variable_cv_5.append(len(item_cv))\n",
    "    \n",
    "    if (all(x in item_cub for x in [0, 1, 2, 3, 4]) == True):\n",
    "        \n",
    "        ind_variable_cub_5 += 1\n",
    "    \n",
    "    if (all(x in item_cv for x in [0, 1, 2, 3, 4]) == True):\n",
    "        \n",
    "        ind_variable_cv_5  += 1\n",
    "\n",
    "\n",
    "print(\"on average CV select\",np.mean(num_variable_cv_5),\"variables\")\n",
    "if (ind_variable_cv_5 >= 0.9 * len(selection_cub_seq_5)):\n",
    "    print(\"in all 40 simulations of CV, 90% of 40 select the first 5 variables\")\n",
    "    \n",
    "print(\"on average minimizing the upper bounds select\",np.mean(num_variable_cub_5),\"variables\")\n",
    "if (ind_variable_cub_5 >= 0.9 * len(selection_cub_seq_5)):\n",
    "    print(\"in all 40 simulations of CUB, 90% of 40 select the first 5 variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the variance of the number of the variable selected by CV is 2.5499305555555556\n",
      "the variance of the number of the variable selected by CUB is 1.286388888888889\n"
     ]
    }
   ],
   "source": [
    "print(\"the variance of the number of the variable selected by CV is\",np.var(num_variable_cv_5))\n",
    "print(\"the variance of the number of the variable selected by CUB is\",np.var(num_variable_cub_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 the repeated simulation of the upper bounds for $n = 100$.\n",
    "\n",
    "\n",
    "Finally we repeat the $n=100$ simulation 20 times using different random seeds. All the plots are attached after the code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can skip the following computation if you already load  \"CVbounds_table.p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## average test error bounds for the lasso\n",
    "## total sample size : 100\n",
    "\n",
    "n3 = 50; test_size_3 = n3; np.random.seed(50) #n3 is the size of each fold\n",
    "\n",
    "# generate data\n",
    "\n",
    "lambda_cv_seq_3 = list(); lambda_cub_seq_3 = list()\n",
    "selection_cv_seq_3 = list();selection_cub_seq_3 = list()\n",
    "\n",
    "for i in range(0, end_seed, step_seed):\n",
    "    \n",
    "    np.random.seed(i)\n",
    "\n",
    "    X13, Y13 = data_gen(n3, n_dim, n_info, cov_noise)\n",
    "\n",
    "    X23, Y23 = data_gen(n3, n_dim, n_info, cov_noise)\n",
    "\n",
    "    # training the model\n",
    "    \n",
    "    lambda_cv_3, lambda_cub_3 = error_bounds(X13, Y13, X23, Y23, test_size_3, varpi, n_iter, n_val, n_dim, n_info, cov_noise, start, end, step)\n",
    "\n",
    "    cub_selection3_1, cub_selection3_2, cv_selection3_1, cv_selection3_2, cub_norm3_1, cub_norm3_2, cv_norm3_1, cv_norm3_2 = lasso_norm(X13, Y13, X23, Y23, lambda_cv_3, lambda_cub_3)\n",
    "    \n",
    "    selection_cub_seq_3.append(cub_selection3_1); selection_cub_seq_3.append(cub_selection3_2)\n",
    "    selection_cv_seq_3.append(cv_selection3_1); selection_cv_seq_3.append(cv_selection3_2)\n",
    "    \n",
    "    lambda_cub_seq_3.append(cub_norm3_1); lambda_cub_seq_3.append(cub_norm3_2)\n",
    "    lambda_cv_seq_3.append(cv_norm3_1); lambda_cv_seq_3.append(cv_norm3_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f3 = plt.figure()\n",
    "plt.hist(lambda_cub_seq_3,alpha=0.5, label='the $L_1$ norm of the lasso based on $\\lambda_{cub}$')\n",
    "plt.hist(lambda_cv_seq_3, alpha=0.5, label='the $L_1$ norm of the lasso based on $\\lambda_{CV}$')\n",
    "#pt.legend(loc=0)\n",
    "plt.xlim(17, 24)\n",
    "plt.ylim(0, 33)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "f3.savefig(\"n=100.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on average CV select 11.591666666666667 variables\n",
      "in all 40 simulations of CV, 90% of 40 select the first 5 variables\n",
      "on average minimizing the upper bounds select 11.166666666666666 variables\n",
      "in all 40 simulations of CUB, 90% of 40 select the first 5 variables\n"
     ]
    }
   ],
   "source": [
    "num_variable_cv_3  = list(); ind_variable_cv_3  = 0\n",
    "num_variable_cub_3 = list(); ind_variable_cub_3 = 0\n",
    "\n",
    "for i in range(len(selection_cub_seq_3)):\n",
    "    \n",
    "    item_cub = selection_cub_seq_3[i]; item_cv = selection_cv_seq_3[i]\n",
    "    num_variable_cub_3.append(len(item_cub)); num_variable_cv_3.append(len(item_cv))\n",
    "    \n",
    "    if (all(x in item_cub for x in [0, 1, 2, 3, 4]) == True):\n",
    "        \n",
    "        ind_variable_cub_3 += 1\n",
    "    \n",
    "    if (all(x in item_cv for x in [0, 1, 2, 3, 4]) == True):\n",
    "        \n",
    "        ind_variable_cv_3  += 1\n",
    "\n",
    "print(\"on average CV select\",np.mean(num_variable_cv_3),\"variables\")\n",
    "if (ind_variable_cv_3 >= 0.9 * len(selection_cub_seq_3)):\n",
    "    print(\"in all 40 simulations of CV, 90% of 40 select the first 5 variables\")\n",
    "    \n",
    "print(\"on average minimizing the upper bounds select\",np.mean(num_variable_cub_3),\"variables\")\n",
    "if (ind_variable_cub_3 >= 0.9 * len(selection_cub_seq_3)):\n",
    "    print(\"in all 40 simulations of CUB, 90% of 40 select the first 5 variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the variance of the number of the variable selected by CV is 10.441597222222223\n",
      "the variance of the number of the variable selected by CUB is 9.488888888888887\n"
     ]
    }
   ],
   "source": [
    "print(\"the variance of the number of the variable selected by CV is\",np.var(num_variable_cv_3))\n",
    "print(\"the variance of the number of the variable selected by CUB is\",np.var(num_variable_cub_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## last step: save everything into a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"CVbounds_table.p\", \"wb\") as f:\n",
    "    pickle.dump( lambda_cv_seq, f)\n",
    "    pickle.dump( lambda_cub_seq, f)\n",
    "    pickle.dump( selection_cv_seq, f)\n",
    "    pickle.dump( selection_cub_seq, f)\n",
    "    \n",
    "    pickle.dump( lambda_cv_seq_5 , f)\n",
    "    pickle.dump( lambda_cub_seq_5 , f)\n",
    "    pickle.dump( selection_cv_seq_5 , f)\n",
    "    pickle.dump( selection_cub_seq_5 , f)\n",
    "    \n",
    "    pickle.dump( lambda_cv_seq_3 , f)\n",
    "    pickle.dump( lambda_cub_seq_3 , f)\n",
    "    pickle.dump( selection_cv_seq_3 , f)\n",
    "    pickle.dump( selection_cub_seq_3 , f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
