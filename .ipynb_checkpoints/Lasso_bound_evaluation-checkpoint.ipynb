{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate data\n",
    "\n",
    "def data_gen(sample_size, n_dim, n_info, cov_noise):\n",
    "    \n",
    "    a = np.ones((n_dim, n_dim)) * 0.5; A = np.eye(n_dim)*0.5\n",
    "\n",
    "    cov_x = a + A; mean_x = np.zeros(n_dim)\n",
    "\n",
    "    X = np.random.multivariate_normal(mean_x, cov_x, sample_size)\n",
    "\n",
    "    beta_info = np.arange(1,n_info + 1)\n",
    "    beta = np.concatenate((beta_info, np.zeros(n_dim-n_info)), axis = 0)\n",
    "\n",
    "    noise = np.random.normal(0, cov_noise, sample_size); \n",
    "    noise.shape = (sample_size, 1); beta.shape = (1, n_dim) \n",
    "\n",
    "    Y = np.inner(X,beta) + noise \n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate rademacher variables\n",
    "\n",
    "def rade_generator( sample_size ):\n",
    "    \n",
    "    ans = np.random.randint(2, size=sample_size)\n",
    "    rade = (ans - 0.5) * 2\n",
    "    \n",
    "    return rade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimating the empirical rademacher complexity\n",
    "\n",
    "def emp_rc(sample_size, n_iter, reg, n_dim, n_info, cov_noise):\n",
    "    \n",
    "    X, Y = data_gen(sample_size, n_dim, n_info, cov_noise)\n",
    "    \n",
    "    cond = np.zeros([n_iter,1])\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        \n",
    "        rade = rade_generator(sample_size)\n",
    "        reg.fit(X, rade) ; rade_pre = reg.predict(X)\n",
    "        rade_pre = np.matrix(rade_pre); cond[i,0] = np.dot(rade_pre, rade)/ sample_size\n",
    "    \n",
    "    emp_rc = np.mean(cond) * 2 \n",
    "   \n",
    "    return emp_rc, cond\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimating the rademacher complexity\n",
    "\n",
    "def est_rc(sample_size, n_iter, reg, n_dim, n_info, cov_noise):\n",
    "      \n",
    "    cond_rc = np.zeros([n_iter])\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        \n",
    "        emp_rc_in, _ = emp_rc(sample_size, n_iter, reg, n_dim, n_info, cov_noise)\n",
    "\n",
    "        cond_rc[i] = emp_rc_in\n",
    "\n",
    "    est_rc = np.mean(cond_rc)\n",
    "    \n",
    "    return est_rc, cond_rc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establishing the upper bound for classifiers\n",
    "\n",
    "def rc_bound(n, M, varpi, K, rc_1, rc_2, ave_in_sample_error):\n",
    "    \n",
    "    ub = ave_in_sample_error + rc_1 + rc_2 + 2 * M * np.sqrt( np.log( 1 / varpi ) / ( n / K ) )\n",
    "    \n",
    "    return ub  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costs(X_train, Y_train, clf):\n",
    "    \n",
    "    Y_pred_train = clf.predict(X_train); Y_pred_train.shape = Y_train.shape\n",
    "    loss_train = mean_squared_error(Y_train, Y_pred_train)\n",
    "    \n",
    "    res = np.var(Y_train - Y_pred_train)\n",
    "    \n",
    "    return loss_train, res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_bounds(X_train_1, Y_train_1, X_train_2, Y_train_2, \n",
    "              test_size, varpi, n_iter, n_val, n_dim, n_info, cov_noise, start, end, step):\n",
    "    \n",
    "    ##set the placeholders \n",
    "    alpha = np.arange(start, end, step)         # the seq of penalty parameters\n",
    "    error_matrix = np.zeros((n_val, len(alpha)))  # the n_val out-of-sample errors for each model\n",
    "    upper_array  = np.zeros(len(alpha))         # the seq of test errors for all models\n",
    "    train_array  = np.zeros(len(alpha))         # the seq of trainig errors for all models\n",
    "    test_array   = np.zeros(len(alpha))         # the seq of test errors for all models\n",
    "    perc_array   = np.zeros(len(alpha))\n",
    "    \n",
    "    for l in range(len(alpha)):\n",
    "        \n",
    "        #given the value of alpha\n",
    "        \n",
    "        #training classifiers\n",
    "        clf  = Lasso(alpha = alpha[l])\n",
    "        clf1 = Lasso(alpha = alpha[l]); clf1.fit(X_train_1, Y_train_1)\n",
    "        clf2 = Lasso(alpha = alpha[l]); clf2.fit(X_train_2, Y_train_2)\n",
    "        clf3 = Lasso(alpha = alpha[l]); clf2.fit(X_train_2, Y_train_2)\n",
    "    \n",
    "        #compute training errors\n",
    "        loss_train_1, _ = costs(X_train_1, Y_train_1, clf1)\n",
    "        loss_train_2, _ = costs(X_train_2, Y_train_2, clf2)\n",
    "        train_array[l] = (loss_train_1 + loss_train_2)/2\n",
    "        \n",
    "        #compute test errors\n",
    "        loss_test_1, res_var_1 = costs(X_train_2, Y_train_2, clf1)\n",
    "        loss_test_2, res_var_2 = costs(X_train_1, Y_train_1, clf2)\n",
    "        test_array[l] = (loss_test_1 + loss_test_2)/2\n",
    "        res_var = (res_var_1 + res_var_2)/2\n",
    "\n",
    "        #container of the validation errors\n",
    "        error_val = np.zeros(n_val); \n",
    "\n",
    "        for i in range(n_val):\n",
    "\n",
    "            X_val_1, Y_val_1 = data_gen(test_size, n_dim, n_info, cov_noise)\n",
    "            X_val_2, Y_val_2 = data_gen(test_size, n_dim, n_info, cov_noise)\n",
    "\n",
    "            Y_pred_val_1 = clf1.predict(X_val_1) ; loss_test_val_1 = mean_squared_error(Y_val_1 , Y_pred_val_1)\n",
    "            Y_pred_val_2 = clf2.predict(X_val_2) ; loss_test_val_2 = mean_squared_error(Y_val_2 , Y_pred_val_2)\n",
    "\n",
    "            error_val[i] = (loss_test_val_1 + loss_test_val_2)/2\n",
    "        \n",
    "        error_matrix[:,l] = error_val; M = np.sqrt(res_var)\n",
    "        \n",
    "        sorted_error = np.sort(error_val); perc_array[l] = sorted_error[-1*round(n_val*0.1)]\n",
    "\n",
    "        # estimating the RC_test and RC_train\n",
    "        est_rc_train, cond_rc_SVC_train = est_rc(test_size, n_iter, clf, n_dim, n_info, cov_noise)\n",
    "        est_rc_test, cond_rc_SVC_test = est_rc(test_size, n_iter, clf, n_dim, n_info, cov_noise)\n",
    "\n",
    "        #establish the upper bounds\n",
    "        upper_array[l] = rc_bound( 2 * test_size, M, varpi, K, est_rc_train, est_rc_test, train_array[l])\n",
    "        \n",
    "        print(\"Model\", int(l+1), \" computed out of \", len(alpha))\n",
    "        \n",
    "    \n",
    "    return error_matrix, upper_array, train_array, test_array, perc_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_plot(alpha, error_matrix, upper_array, train_array, test_array, perc_array):\n",
    "    \n",
    "    #set the tick of the x axis\n",
    "    index_plot = np.arange(1, len(alpha)+1); my_xticks = list(map(str,alpha))\n",
    "    \n",
    "    #set the figure\n",
    "    f = plt.figure()\n",
    "    \n",
    "    #multiple boxplot\n",
    "    plt.boxplot(error_matrix, widths = 0.25)\n",
    "\n",
    "    #plot the average training error, average test error, 90% percentile and 90% upper bound\n",
    "    plt.plot(index_plot, upper_array, color = 'g', label='90% upper bound of the out-of-sample error')\n",
    "    plt.plot(index_plot, train_array, color = 'b', label='average training error')\n",
    "    plt.plot(index_plot, test_array,  color = 'r', label='average test error')\n",
    "    plt.plot(index_plot, perc_array,  color = 'k', label='90% percentile')\n",
    "\n",
    "    #set the legend, label, grid and tick\n",
    "    legend = plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., shadow=True)\n",
    "    frame = legend.get_frame()\n",
    "\n",
    "    plt.xticks(index_plot, my_xticks)\n",
    "    plt.grid(axis='y', linestyle='-')\n",
    "    plt.title('the out-of-sample error of the lasso')\n",
    "    plt.xlabel('value of $\\lambda$')\n",
    "    plt.ylabel('value of the error')\n",
    "\n",
    "    plt.show()\n",
    "    f.savefig(\"lasso_plot.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = x1 + 2 * x2 + 3 * x3 + 4 * x4 + 5 * x5 + 0 * x6 + 0 * x7 + 0 * x8 + 0 * x9 + 0 * x10 + e\n",
    "\n",
    "n = 100; test_size = n\n",
    "\n",
    "varpi = 0.1; K = 2\n",
    "\n",
    "n_iter = 64; n_val = 1000\n",
    "\n",
    "n_dim = 10; n_info = 5; cov_noise = 1\n",
    "\n",
    "start = 0.1; end = 0.5; step = 0.05; alpha = np.arange(start, end, step)\n",
    "\n",
    "# generate data\n",
    "\n",
    "X1, Y1 = data_gen(n, n_dim, n_info, cov_noise)\n",
    "\n",
    "X2, Y2 = data_gen(n, n_dim, n_info, cov_noise)\n",
    "\n",
    "\n",
    "# training the model\n",
    "error_matrix, upper_array, train_array, test_array, perc_array = cv_bounds(X1, Y1, X2, Y2, test_size, varpi, n_iter, \n",
    "                                                               n_val, n_dim, n_info, cov_noise, start, end, step)\n",
    "\n",
    "# plot the figure\n",
    "error_plot(alpha, error_matrix, upper_array, train_array, test_array, perc_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
